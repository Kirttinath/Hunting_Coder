{
  "title": "How to learn Coding",
  "content": "<h2>1) Familiarize Yourself with Computer Architecture and Data Basics</h2> <p>One of the wonderful things about modern programming languages is that they enable us to create fancy applications without worrying about the nitty-gritty details of the hardware behind the scenes (for the most part).</p> <p>This is called&nbsp;<strong>abstraction</strong>&nbsp;&ndash; the ability to work with higher-level tools (in this case programming languages) that simplify and narrow down the required scope of our understanding and skills.</p> <p>However, that doesn&#39;t mean it&#39;s useless to know the basics of the metal that your code is executing on. At the very least, being aware of a few tidbits will help you navigate workplace conversations about high CPU and memory usage.</p> <p>So, here is a bare minimum of computer architecture basics to get you started:</p> <p>Your computer&#39;s most important parts live on&nbsp;<strong>microchips</strong>&nbsp;(also known as&nbsp;<strong>integrated circuits</strong>).</p> <p>Microchips rely on an electrical component called a&nbsp;<strong>transistor&nbsp;</strong>to function. Transistors are tiny electrical switches that are either off (0) or on (1) at any given time. A single microchip can contain millions or billions of tiny transistors embedded on it.</p> <p>Most modern computers have a microchip called the&nbsp;<strong>Central Processing Unit (CPU)</strong>. You can think of it as the computer&rsquo;s brain. It handles most of the number crunching and logical tasks that the computer performs.</p> <p>Each CPU has something called an&nbsp;<strong>instruction set</strong>, which is a collection of binary (zeros and ones) commands that the CPU understands. Luckily, we don&#39;t really need to worry about these as software devs! That is the power of abstraction.</p> <p>If the CPU is the logical center of the brain, it is useful to have memory as well to store information temporarily or for the long term.</p> <p>Computers have&nbsp;<strong>Random Access Memory (RAM)</strong>&nbsp;as &quot;working memory&quot; (or short-term memory) to store information that is actively being used by running programs.</p> <p>RAM is made up of a collection of&nbsp;<strong>memory addresses</strong>, which can be used to store bits of data. In older languages like C, programmers do have access to working directly with memory addresses using a feature called&nbsp;<strong>pointers</strong>, but this is rare in more modern languages.</p> <p>Finally, we&#39;ll touch on a component you&#39;re surely familiar with &ndash; the hard drive. In our analogy of the brain, this represents long-term memory. A hard drive is an internal or external device that stores data that should persist even after the computer is turned off.</p> <p>Before moving on to more details about programming languages, let&#39;s spend a second talking about data. But what exactly do we mean by the word&nbsp;<strong>data</strong>?</p> <p>At a high level, we think of things like text documents, images, videos, emails, files, and folders. These are all high-level data structures that we create and save on our computers every day.</p> <p>But underneath the hood, a computer chip (like a CPU or RAM chip) has no idea what an &quot;image&quot; or a &quot;video&quot; is.</p> <p>From a chip&rsquo;s perspective, all of these structures are stored as long sequences of ones and zeros. These ones and zeros are called&nbsp;<strong>bits</strong>.</p> <p>Bits are commonly stored in a set of eight at a time, known as a&nbsp;<strong>byte</strong>. A byte is simply a sequence of eight bits, such as&nbsp;<code>00000001</code>,&nbsp;<code>01100110</code>, or&nbsp;<code>00001111</code>. Representing information in this way is called a&nbsp;<strong>binary representation</strong>.</p> <h2>2) Learn How Programming Languages Work</h2> <p>In the previous section, we mentioned that most computers rely on a CPU, and a CPU can understand a specific set of instructions in the form of ones and zeros.</p> <p>Therefore, we could theoretically write code that tells the CPU what to do by stringing together long sequences of ones and zeros in a form the CPU understands. Instructions written in binary form like this are called&nbsp;<strong>machine code</strong>.</p> <p>Sounds horrible to work with, doesn&#39;t it? Well it probably is, but I wouldn&#39;t know since I mostly use higher-level programming languages like JavaScript, Python, and Java.</p> <p>A&nbsp;<strong>higher-level programming language</strong>&nbsp;provides a set of human-readable keywords, statements, and syntax rules that are much simpler for people to learn, debug, and work with.</p> <p>Programming languages provide a means of bridging the gap between the way our human brains understand the world and the way computer brains (CPUs) understand the world.</p> <p>Ultimately, the code that we write needs to be translated into the binary instructions (machine code) that the CPU understands.</p> <p>Depending on the language you choose, we say that your code is either&nbsp;<strong>compiled</strong>&nbsp;or&nbsp;<strong>interpreted</strong>&nbsp;into machine code capable of being executed by your CPU. Most programming languages include a program called a&nbsp;<strong>compiler</strong>&nbsp;or an&nbsp;<strong>interpreter</strong>&nbsp;which performs this translation step.</p> <p>Just to give a few examples &ndash; JavaScript and Python are interpreted languages while Java is a compiled language. Whether a language is compiled or interpreted (or some combination of the two) has implications for developer convenience, error handling, performance, and other areas, but we won&#39;t get into those details here.</p> <h2>3) Understand How the Internet Works</h2> <p>Whatever type of programming you aspire to do, you&#39;ll run into situations where it helps to know how computers interact with each other. This typically occurs over the Internet.</p> <p>The Internet is nothing more than a global collection of connected computers. In other words, it is a global network. Each computer in the network agrees on a set of rules that enable them to talk to each other. To a computer, &quot;talking&quot; means transferring data.</p> <p>As we discussed in the previous section, all types of data &ndash; web pages, images, videos, emails, and so on &ndash; can all be represented as ones and zeros.</p> <p>Therefore, you can think of the Internet as a very large set of computers that can transfer ones and zeros amongst themselves, in a way that preserves the meaning of that data. The Internet is nothing more than a digital conversation medium.</p> <p>If the Internet is just a big conversation arena, let&rsquo;s define the conversation participants.</p> <p>First, an analogy: most human conversations require at least two participants. In most cases, one person initiates the conversation and the other person responds, assuming they are both present and available.</p> <p>In Internet speak, the computer initiating the conversation is called the&nbsp;<strong>client</strong>. The computer responding or answering is called the&nbsp;<strong>server</strong>.</p> <p>For example, let&rsquo;s say you open a web browser and go to &quot;www.google.com&quot;. In this scenario, your web browser is the client. By extension, you can also think of the computer you are working on as the client.</p> <p>In a more abstract sense, YOU are the client because you are the one initiating the conversation. By typing &quot;www.google.com&quot; into the search bar and clicking &lt;ENTER&gt;, your browser is requesting to start a conversation with one of Google&rsquo;s computers.</p> <p>Google&rsquo;s computer is called the server. It responds by sending the data required to display Google&rsquo;s web page in your browser. And voil&agrave;! Google&rsquo;s web page appears in front of your eyes. All Internet data transfers utilize this sort of client/server relationship.</p>",
  "author": "Gourav Rana",
  "metadesc": "Learn coding in few minutes.I will teach you in few minutes",
  "slug": "how-to-learn-coding"
}
